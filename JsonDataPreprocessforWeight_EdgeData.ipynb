{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rd2PFhWh5dZm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process the JSON data into a format suitable for a heatmap\n",
        "def process_json_for_heatmap(json_filepath, output_filepath):\n",
        "    # Load the data\n",
        "    with open(json_filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Extract the numeric part of the Source and Target\n",
        "    df['Source'] = df['Source'].str.extract('(\\d+)').astype(int)\n",
        "    df['Target'] = df['Target'].str.extract('(\\d+)').astype(int)\n",
        "\n",
        "    # Create a pivot table\n",
        "    heatmap_data = df.pivot_table(index='Source', columns='Target', values='Weight', fill_value=0)\n",
        "\n",
        "    # Normalize the weights by sqrt as mentioned earlier, if needed\n",
        "    # heatmap_data = np.sqrt(heatmap_data)\n",
        "\n",
        "    # Fill in the missing values with 0s and ensure the matrix is square\n",
        "    max_index = max(heatmap_data.index.union(heatmap_data.columns))\n",
        "    heatmap_matrix = heatmap_data.reindex(index=range(max_index+1), columns=range(max_index+1), fill_value=0)\n",
        "\n",
        "    # Convert the matrix back to a format that can be used for heatmap (list of dictionaries)\n",
        "    processed_data = heatmap_matrix.stack().reset_index().rename(columns={0: 'Weight'})\n",
        "    processed_data = processed_data[processed_data['Weight'] > 0].to_dict(orient='records')\n",
        "\n",
        "    # Save the processed data to a JSON file\n",
        "    with open(output_filepath, 'w') as f:\n",
        "        json.dump(processed_data, f, indent=4)\n",
        "\n",
        "# Replace 'path_to_your_large_json_file.json' with the path to your JSON file\n",
        "# Replace 'output_processed_heatmap_data.json' with the path where you want the processed data to be saved\n",
        "process_json_for_heatmap('/content/WT_BS_Edge.json', '/content/WT_BS_Edge_processed.json')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Including Interaction attribute"
      ],
      "metadata": {
        "id": "V-em8vlBmRYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "# Function to process the JSON data into a format suitable for a heatmap\n",
        "def process_json_for_heatmap(json_filepath, output_filepath, chromosome_bins):\n",
        "    # Load the data\n",
        "    with open(json_filepath, 'r') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Extract the numeric part of the Source and Target\n",
        "    df['Source'] = df['Source'].astype(int)\n",
        "    df['Target'] = df['Target'].astype(int)\n",
        "\n",
        "    # Determine interaction type\n",
        "    def determine_interaction(source, target, bins):\n",
        "        for start, end in bins:\n",
        "            if start <= source <= end and start <= target <= end:\n",
        "                return 1  # Intra-interaction\n",
        "        return 2  # Inter-interaction\n",
        "\n",
        "    df['Interaction'] = df.apply(lambda row: determine_interaction(row['Source'], row['Target'], chromosome_bins), axis=1)\n",
        "\n",
        "    # Create a pivot table\n",
        "    heatmap_data = df.pivot_table(index='Source', columns='Target', values='Weight', fill_value=0)\n",
        "\n",
        "    # Fill in the missing values with 0s and ensure the matrix is square\n",
        "    max_index = max(heatmap_data.index.union(heatmap_data.columns))\n",
        "    heatmap_matrix = heatmap_data.reindex(index=range(max_index+1), columns=range(max_index+1), fill_value=0)\n",
        "\n",
        "    # Convert the matrix back to a format that can be used for heatmap (list of dictionaries)\n",
        "    processed_data = heatmap_matrix.stack().reset_index().rename(columns={0: 'Weight'})\n",
        "    processed_data = processed_data[processed_data['Weight'] > 0]\n",
        "\n",
        "    # Add the interaction data back to the processed data\n",
        "    processed_data = processed_data.merge(df[['Source', 'Target', 'Interaction']], on=['Source', 'Target'], how='left')\n",
        "\n",
        "    processed_data = processed_data.to_dict(orient='records')\n",
        "\n",
        "    # Save the processed data to a JSON file\n",
        "    with open(output_filepath, 'w') as f:\n",
        "        json.dump(processed_data, f, indent=4)\n",
        "\n",
        "# Example chromosome bin ranges (these need to be defined as per the specific data)\n",
        "chromosome_bins = [(1, 404)]\n",
        "\n",
        "# Replace 'path_to_your_large_json_file.json' with the path to your JSON file\n",
        "# Replace 'output_processed_heatmap_data.json' with the path where you want the processed data to be saved\n",
        "process_json_for_heatmap('/content/WT_BS_Edge_processed.json', '/content/WT_BS_Edge_processed_with_interaction.json', chromosome_bins)\n"
      ],
      "metadata": {
        "id": "pdav2iYe5kfo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "shrW7Dy25kc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FwnX-kQ45kaD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pf3YLi7v5kW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v2G5MaCY5kUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aup2sa9c5kRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vqfb-ocS5kPI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NC3_mlr65kMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JI9ohf8D5kG2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UX89zJDN5kEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2eZJKks5kB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UqNf-j1v5j-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_aULy_RB5j8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8pe2vW7a5j5k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}